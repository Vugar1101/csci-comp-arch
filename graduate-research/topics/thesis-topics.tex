\documentclass{beamer}
\usetheme{Madrid}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{graphicx}
\usepackage{hyperref}

% Setting the title, author, and date
\title{2025-2026 Master Thesis Topics in OpenMP and ML/AI}
\author{John Burns, ADA University}
\date{July 16, 2025}

% Beginning the document
\begin{document}

% Creating the title slide
\begin{frame}
    \titlepage
\end{frame}

% Introducing the overview slide
\begin{frame}{Overview}
    \tableofcontents
\end{frame}

% Section for Thesis Topic 1
\section{Introduction}
\begin{frame}{About myself and my research interests}
    \begin{itemize}
	\item My PhD was in the field of large-scale (at the time!) modelling and simulation
        \item From my industry background I also have an interest in performance, reliability and scalability
        \item From my teaching I have an interest in Computer Architecture and Distributed Systems
        \item I welcome any suggestions you may have in the intersection of these fields
        \item I am very interested in OpenMP projects - from code and performance analysis through to application developement
	\item As Energy Efficiency and Optimization is so important now, why not use ARM64 cloud platforms for these research topics.
    \end{itemize}
\end{frame}

\section{Topic 1: Optimizing Deep Neural Network Training with OpenMP}
\begin{frame}{Topic 1: Optimizing Deep Neural Network Training with OpenMP}
\begin{itemize}
    \item \textbf{Objective}: Enhance the performance of deep neural network (DNN) training by parallelizing compute-intensive operations (e.g., matrix multiplications, gradient computations) using OpenMP on multi-core CPUs.
    \item \textbf{Description}: Implement and evaluate OpenMP-based parallelization strategies for DNN training in frameworks like PyTorch or TensorFlow. Focus on optimizing data parallelism and model parallelism for large-scale datasets.
    \item \textbf{Relevance}: Reduces training time for DNNs, making them more feasible for resource-constrained environments.
    \item \textbf{Methodology}: Develop parallelized versions of backpropagation and optimization algorithms, benchmark performance on multi-core systems, and compare with GPU-based implementations.
\end{itemize}
\end{frame}

\begin{frame}{Topic 1: References}
\begin{itemize}
    \item \textbf{Rakhimov, M., et al.}, "Parallel Approaches in Deep Learning: Use Parallel Computing," \textit{International Conference on Future Networks and Distributed Systems (ICFNDS)}, 2023. \url{https://doi.org/10.1145/3644713.3644738}
    \item \textbf{Zhang, Y., et al.}, "Exploiting Parallelism Opportunities with Deep Learning Frameworks," \textit{ACM Transactions on Architecture and Code Optimization}, vol. 18, no. 1, 2020. \url{https://doi.org/10.1145/3431388}

\end{itemize}
\end{frame}

\section{Topic 2: Help! Advising OpenMP Parallelization}
\begin{frame}{Topic 2: Help! Advising OpenMP Parallelization}
\begin{itemize}
    \item \textbf{Objective}: Reduce the programming burden of OpenMP annotation tasks.
    \item \textbf{Description}: Research DeepTyper-based learning architecture for advising OpenMP Annotation / Parallelization.
    \item \textbf{Relevance}: Valuable research to reduce the human workload in OpenMP development.
    \item \textbf{Methodology}: Using CodeT5+ and DeepTyper approaches.
\end{itemize}
\end{frame}

\begin{frame}{Topic 2: References}
\begin{itemize}
    \item \textbf{Pornmaneerattanatri, S., et al.,} "Automatic Parallelization with CodeT5+: A Model for Generating OpenMP Directives," \textit{International Workshop on Large Language Models and HPC}, 2024.
    \item \textbf{Shen, Y., et al.}, "A machine learning method to variable classification in OpenMP," \textit{Concurrency and Computation: Practice and Experience}, 2023. \url{https://doi.org/10.1002/cpe.7746}
    \item \textbf{Kadosh, T., et al.}, "Advising OpenMP Parallelization via A Graph-Based Approach with Transformers"
    OpenMP: Advanced Task-Based, Device and Compiler Programming (pp.3-17) 
\end{itemize}
\end{frame}

\section{Topic 3: Parallel Graph Neural Networks with OpenMP}
\begin{frame}{Topic 3: Parallel Graph Neural Networks with OpenMP}
\begin{itemize}
    \item \textbf{Objective}: Enhance the scalability of graph neural networks (GNNs) for large-scale graph data using OpenMP parallelization.
    \item \textbf{Description}: Implement OpenMP-based parallel processing for GNN operations like message passing and aggregation, targeting applications in social networks or bioinformatics.
    \item \textbf{Relevance}: GNNs are computationally expensive for large graphs; OpenMP can improve training and inference speed on multi-core systems.
    \item \textbf{Methodology}: Develop parallel GNN algorithms, test on datasets like OGB (Open Graph Benchmark), and compare performance with serial implementations.
\end{itemize}
\end{frame}

\begin{frame}{Topic 3: References}
\begin{itemize}
    \item \textbf{Meng, Z., et al.}, "OpenMP Parallelization and Optimization of Graph-Based Machine Learning Algorithms," \textit{International Workshop on OpenMP (IWOMP)}, 2016
    \item \textbf{Zhou, J., et al.}, "Graph Neural Networks: A Review of Methods and Applications," \textit{AI Open}, vol. 1, 2020. \url{https://doi.org/10.1016/j.aiopen.2021.01.001}
\end{itemize}
\end{frame}

\section{Topic 4: Energy-Efficient ML Model Training with OpenMP}
\begin{frame}{Topic 4: Energy-Efficient ML Model Training with OpenMP}
\begin{itemize}
    \item \textbf{Objective}: Develop energy-efficient ML training pipelines using OpenMP to optimize resource utilization on multi-core CPUs.
    \item \textbf{Description}: Investigate OpenMP-based parallelization to reduce energy consumption in ML tasks, focusing on dynamic thread management and workload balancing.
    \item \textbf{Relevance}: Energy efficiency is critical for sustainable AI; OpenMP can optimize CPU-based training for green computing.
    \item \textbf{Methodology}: Implement energy-aware parallel algorithms, measure energy consumption using 
PowerPoint, and compare with traditional ML training pipelines.
\end{itemize}
\end{frame}

\begin{frame}{Topic 4: References}
\begin{itemize}
    \item \textbf{Garcia, A., et al.}, "DNN Is Not All You Need: Parallelizing Non-Neural ML Algorithms on Ultra-Low-Power IoT Processors," \textit{ACM Transactions on Embedded Computing Systems}, 2023. \url{https://doi.org/10.1145/3570152}
    \item \textbf{Strubell, E., et al.}, "Energy and Policy Considerations for Deep Learning," \textit{ACM Computing Surveys}, vol. 53, no. 3, 2020. \url{https://doi.org/10.1145/3372822}

\end{itemize}
\end{frame}

\section{Topic 5: OpenMP enhancement of R Selected Contributed Packages}
\begin{frame}{Topic 5: OpenMP enhancement of R Selected Contributed Packages}
\begin{itemize}
    \item \textbf{Objective}:Enhance the performance of selected package using OpenMP.
    \item \textbf{Description}: Today there are over 22000 Contributed Packages in cran-r.
    \item \textbf{Relevance}: R is one of the most popular languages for data science, with 31\% 
    of data scientists regularly using it, according to a 2025 source..
    \item \textbf{Methodology}: Select a popular R contrib package, (eg ggplot2, dplyr, tdyr etc)
    or some other package of your choice. Ccompile locally and test with and without
    OpenMP directives. Benchmark performance and contribute to the community
\end{itemize}
\end{frame}

\begin{frame}{Topic 5: References}
\begin{itemize}
    \item Start here: \texttt{install.packages("packageRank"}

\end{itemize}
\end{frame}

\begin{frame}{Conclusion}
\begin{itemize}
    \item These topics combine OpenMP's parallel computing capabilities with cutting-edge ML/AI challenges.
    \item They address performance, scalability, privacy, and sustainability in AI systems.
    \item Obviously knowledge/interest in C and Parallel Computing
    is a requirement
    \item A good place to start: OpenMP Architecture Review Board, "OpenMP API Specification: Version 5.2," 2024. \url{https://www.openmp.org/specifications/}
\end{itemize}
\end{frame}

\end{document}
