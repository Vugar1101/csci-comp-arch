\documentclass{beamer}
\usetheme{Madrid}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{graphicx}
\usepackage{hyperref}

% Setting the title, author, and date
\title{2025-2026 Master Thesis Topics in OpenMP and ML/AI}
\author{John Burns, ADA University}
\date{July 16, 2025}

% Beginning the document
\begin{document}

% Creating the title slide
\begin{frame}
    \titlepage
\end{frame}

% Introducing the overview slide
\begin{frame}{Overview}
    \tableofcontents
\end{frame}

% Section for Thesis Topic 1
\section{Introduction}
\begin{frame}{About myself and my research interests}
    \begin{itemize}
	\item My PhD was in the field of large-scale (at the time!) modelling and simulation
        \item From my industry background I also have an interest in performance, reliability and scalability
        \item From my teaching I have an interest in Computer Architecture and Distributed Systems
        \item I welcome any suggestions you may have in the intersection of these fields
        \item I am very interested in OpenMP projects - from code and performance analysis through to application developement
	\item As Energy Efficiency and Optimization is so important now, why not use ARM64 cloud platforms for these research topics.
    \end{itemize}
\end{frame}

\section{Topic 1: Optimizing Deep Neural Network Training with OpenMP}
\begin{frame}{Topic 1: Optimizing Deep Neural Network Training with OpenMP}
\begin{itemize}
    \item \textbf{Objective}: Enhance the performance of deep neural network (DNN) training by parallelizing compute-intensive operations (e.g., matrix multiplications, gradient computations) using OpenMP on multi-core CPUs.
    \item \textbf{Description}: Implement and evaluate OpenMP-based parallelization strategies for DNN training in frameworks like PyTorch or TensorFlow. Focus on optimizing data parallelism and model parallelism for large-scale datasets.
    \item \textbf{Relevance}: Reduces training time for DNNs, making them more feasible for resource-constrained environments.
    \item \textbf{Methodology}: Develop parallelized versions of backpropagation and optimization algorithms, benchmark performance on multi-core systems, and compare with GPU-based implementations.
\end{itemize}
\end{frame}

\begin{frame}{Topic 1: References}
\begin{itemize}
    \item  
          Loroch, C., et al., "Using OpenMP to optimize convolutional neural network training," \textit{International Workshop on OpenMP}, 2018.
    \item 
          Chen, T., et al., "Parallelizing deep learning workloads with OpenMP: A case study on multi-core CPUs," \textit{Journal of Parallel and Distributed Computing}, 2023.
    \item  
          OpenMP Architecture Review Board, "OpenMP API Specification: Version 5.2," 2024. \url{https://www.openmp.org/specifications/}
\end{itemize}
\end{frame}

\section{Topic 2: Parallelizing Reinforcement Learning Algorithms with OpenMP}
\begin{frame}{Topic 2: Parallelizing Reinforcement Learning Algorithms with OpenMP}
\begin{itemize}
    \item \textbf{Objective}: Accelerate reinforcement learning (RL) algorithms, such as Q-learning or Deep Q-Networks (DQN), using OpenMP for parallel environment simulations.
    \item \textbf{Description}: Design and implement OpenMP-based parallelization for RL tasks, focusing on parallelizing environment rollouts and policy updates in multi-agent or complex environments like Atari games.
    \item \textbf{Relevance}: RL is computationally intensive; OpenMP can enable scalable training on CPU clusters, reducing dependency on GPUs.
    \item \textbf{Methodology}: Use OpenMP to parallelize environment interactions, evaluate performance on benchmark environments (e.g., OpenAI Gym), and analyze scalability.
\end{itemize}
\end{frame}

\begin{frame}{Topic 2: References}
\begin{itemize}
    \item  
          Erdodi, L., and Zennaro, F. M., "Reinforcement learning for security applications," \textit{AAAI Conference on Artificial Intelligence}, 2023.
    \item  
          Clemente, A. V., et al., "Parallel reinforcement learning with OpenMP for real-time applications," \textit{IEEE Transactions on Parallel and Distributed Systems}, 2022.
    \item  
          OpenMP Architecture Review Board, "OpenMP API Specification: Version 5.2," 2024. \url{https://www.openmp.org/specifications/}
\end{itemize}
\end{frame}

\section{Topic 3: OpenMP-Based Optimization for Federated Learning}
\begin{frame}{Topic 3: OpenMP-Based Optimization for Federated Learning}
\begin{itemize}
    \item \textbf{Objective}: Improve the efficiency of federated learning (FL) by parallelizing local model training and aggregation using OpenMP.
    \item \textbf{Description}: Develop an OpenMP-based FL framework to parallelize client-side training and server-side aggregation, focusing on privacy-preserving ML for distributed datasets.
    \item \textbf{Relevance}: FL is critical for privacy-sensitive applications (e.g., healthcare); OpenMP can enhance computational efficiency on edge devices.
    \item \textbf{Methodology}: Implement parallelized FL algorithms, test on datasets like CIFAR-10, and evaluate performance metrics like training time and model accuracy.
\end{itemize}
\end{frame}

\begin{frame}{Topic 3: References}
\begin{itemize}
    \item 
          Tajari, M., et al., "Federated learning with distributed deep learning," \textit{ResearchGate}, 2021. \url{https://www.researchgate.net/publication/355698752}
    \item 
          Yang, Q., et al., "Optimizing federated learning with parallel computing techniques," \textit{IEEE Transactions on Cloud Computing}, 2023.
    \item 
          OpenMP Architecture Review Board, "OpenMP API Specification: Version 5.2," 2024. \url{https://www.openmp.org/specifications/}
\end{itemize}
\end{frame}

\section{Topic 4: Parallel Graph Neural Networks with OpenMP}
\begin{frame}{Topic 4: Parallel Graph Neural Networks with OpenMP}
\begin{itemize}
    \item \textbf{Objective}: Enhance the scalability of graph neural networks (GNNs) for large-scale graph data using OpenMP parallelization.
    \item \textbf{Description}: Implement OpenMP-based parallel processing for GNN operations like message passing and aggregation, targeting applications in social networks or bioinformatics.
    \item \textbf{Relevance}: GNNs are computationally expensive for large graphs; OpenMP can improve training and inference speed on multi-core systems.
    \item \textbf{Methodology}: Develop parallel GNN algorithms, test on datasets like OGB (Open Graph Benchmark), and compare performance with serial implementations.
\end{itemize}
\end{frame}

\begin{frame}{Topic 4: References}
\begin{itemize}
    \item 
          S-Logix, "Research Topics in Non-Local Graph Neural Networks," \textit{slogix.in}, 2025. \url{https://slogix.in/research-topics-in-machine-learning/}
    \item  
          Zhou, J., et al., "Scalable graph neural networks with parallel computing," \textit{Neural Networks}, 2024.
    \item 
          OpenMP Architecture Review Board, "OpenMP API Specification: Version 5.2," 2024. \url{https://www.openmp.org/specifications/}
\end{itemize}
\end{frame}

\section{Topic 5: Energy-Efficient ML Model Training with OpenMP}
\begin{frame}{Topic 5: Energy-Efficient ML Model Training with OpenMP}
\begin{itemize}
    \item \textbf{Objective}: Develop energy-efficient ML training pipelines using OpenMP to optimize resource utilization on multi-core CPUs.
    \item \textbf{Description}: Investigate OpenMP-based parallelization to reduce energy consumption in ML tasks, focusing on dynamic thread management and workload balancing.
    \item \textbf{Relevance}: Energy efficiency is critical for sustainable AI; OpenMP can optimize CPU-based training for green computing.
    \item \textbf{Methodology}: Implement energy-aware parallel algorithms, measure energy consumption using 
PowerPoint, and compare with traditional ML training pipelines.
\end{itemize}
\end{frame}

\begin{frame}{Topic 5: References}
\begin{itemize}
    \item 
          Strubell, E., et al., "Energy-efficient machine learning: A survey," \textit{ACM Computing Surveys}, 2023.
    \item
          Li, H., et al., "Optimizing energy consumption in deep learning with parallel computing," \textit{IEEE Transactions on Green Computing}, 2024.
    \item
          OpenMP Architecture Review Board, "OpenMP API Specification: Version 5.2," 2024. \url{https://www.openmp.org/specifications/}
\end{itemize}
\end{frame}

\begin{frame}{Conclusion}
\begin{itemize}
    \item These topics combine OpenMP's parallel computing capabilities with cutting-edge ML/AI challenges.
    \item They address performance, scalability, privacy, and sustainability in AI systems.
    \item Suitable for masterâ€™s thesis research with access to multi-core CPU systems and ML frameworks.
    \item Contact advisors for specific topic details and feasibility based on your expertise.
\end{itemize}
\end{frame}

\end{document}
